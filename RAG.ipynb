{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3e470b-8da2-41e9-b79b-2e953b486970",
   "metadata": {},
   "source": [
    "This Jupyter Notebook builds upon the referenced [Google Colab](https://colab.research.google.com/drive/1QzQYQxH052Rs088rxMQDveRjoVdkdl74?usp=sharing), integrating customized modifications to address the specific requirements of our testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a3f7ca-2d0e-41a1-9a1b-6c67a48c37c3",
   "metadata": {},
   "source": [
    "# 1. Load Multimodal Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84563197-f172-49a1-a347-d9e459694fff",
   "metadata": {},
   "source": [
    "## 1.1. Extract and Partition Text, Tables, and Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b40a1a8-178d-4f09-99d8-00adb764f249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove existing figures\n",
    "!rm -rf ./figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5bde7b-c7a8-49c6-9311-54c216fd6646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "# Directory containing the PDFs\n",
    "pdf_dir = './references'\n",
    "\n",
    "# Collect all PDF files from the directory and subdirectories, excluding hidden ones\n",
    "pdf_files = []\n",
    "for root, dirs, files in os.walk(pdf_dir):\n",
    "    # Exclude hidden directories\n",
    "    dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "    for file in files:\n",
    "        # Exclude hidden files\n",
    "        if file.lower().endswith('.pdf') and not file.startswith('.'):\n",
    "            pdf_files.append(os.path.join(root, file))\n",
    "\n",
    "# Initialize an empty list to hold data from all PDFs\n",
    "data = []\n",
    "\n",
    "# Loop through each PDF file and load it\n",
    "for pdf_file in pdf_files:\n",
    "    print(f'Loading {pdf_file}')\n",
    "    loader = UnstructuredPDFLoader(\n",
    "        file_path=pdf_file,\n",
    "        strategy='hi_res',\n",
    "        extract_images_in_pdf=True,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",     # section-based chunking\n",
    "        max_characters=4000,              # max size of chunks\n",
    "        new_after_n_chars=4000,           # preferred size of chunks\n",
    "        combine_text_under_n_chars=2000,  # combine smaller chunks\n",
    "        mode='elements',\n",
    "        image_output_dir_path='./figures'\n",
    "    )\n",
    "    data.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd705d3-bc74-4d32-bd0d-2c86d896a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "tables = []\n",
    "\n",
    "for doc in data:\n",
    "    if doc.metadata['category'] == 'Table':\n",
    "        tables.append(doc)\n",
    "    elif doc.metadata['category'] == 'CompositeElement':\n",
    "        docs.append(doc)\n",
    "\n",
    "len(docs), len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c31a6b-e383-4756-b465-94cc673e7a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import htmltabletomd\n",
    "\n",
    "for table in tables:\n",
    "    table.page_content = htmltabletomd.convert_table(table.metadata['text_as_html'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b525503d-85fb-4416-8d05-898b73822274",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.2. Examine Loaded Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf0d95-a861-48eb-b87b-e61bc2308a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display, Markdown, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e8d372-6a8b-4891-9ef6-7f5b7e715f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97df5c40-bc2b-4ea3-ad03-de53640d5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tables[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ea936-e100-4fc2-8d61-2f7054117691",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0].metadata['text_as_html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d113d-aae5-4d63-8095-c25879911ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(tables[0].metadata['text_as_html']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a59c9a-932f-4aa7-8ee9-737824f36dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in tables:\n",
    "    print(table.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b485e78-3f56-4a14-acbf-2dfe7f4cabf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -l ./figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72b4ef-f8f9-43fa-8c2e-39241342a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./figures/figure-5-4.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ca0769-a9e5-4cf0-b4fb-1923155fb912",
   "metadata": {},
   "source": [
    "# 2. Connect to LLM through API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b662d-ce16-4b2a-a2e4-6ae9b994cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9830471c-d740-482a-8cca-e0c3d83bde73",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f0b546-2e3d-4ed6-a275-9e53573ef4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatgpt = ChatOpenAI(model_name='gpt-4o', temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5eebd2-9e7b-4eb4-810a-926241aafc7b",
   "metadata": {},
   "source": [
    "# 3. Generate Text-Based Summaries for Multimodal Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08edce5-36ea-489e-ac74-9dad717f5454",
   "metadata": {},
   "source": [
    "## 3.1. Create Text and Table Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7462704f-5c1d-4ac5-8cc7-7f4ad37d5aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt\n",
    "prompt_text = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text particularly for semantic retrieval.\n",
    "These summaries will be embedded and used to retrieve the raw text or table elements.\n",
    "Give a detailed summary of the table or text below that is well optimized for retrieval.\n",
    "For any tables also add in a one line description of what the table is about besides the summary.\n",
    "Do not add additional words like Summary: etc.\n",
    "\n",
    "Table or text chunk:\n",
    "{element}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "summarize_chain = (\n",
    "                    {\"element\": RunnablePassthrough()}\n",
    "                      |\n",
    "                    prompt\n",
    "                      |\n",
    "                    chatgpt\n",
    "                      |\n",
    "                    StrOutputParser() # Extracts the response as text and returns it as a string\n",
    ")\n",
    "\n",
    "# Initialize empty summaries\n",
    "text_summaries = []\n",
    "table_summaries = []\n",
    "\n",
    "text_docs = [doc.page_content for doc in docs]\n",
    "table_docs = [table.page_content for table in tables]\n",
    "\n",
    "text_summaries = summarize_chain.batch(text_docs, {\"max_concurrency\": 5})\n",
    "table_summaries = summarize_chain.batch(table_docs, {\"max_concurrency\": 5})\n",
    "\n",
    "len(text_summaries), len(table_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f24782-8538-4abb-96e8-dbc2e14f5418",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.2. Examine Text and Table Summaries (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a1e4f-e5ea-4258-9eed-bec50ef3d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18840116-269a-419f-ad75-a574ba00e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_summaries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd91cd0-bcf5-476c-a949-554757c38f66",
   "metadata": {},
   "source": [
    "## 3.3. Create Image Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97e35e8-e966-453d-8ba5-80fe57b7f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a function to encode images\n",
    "def encode_image(image_path):\n",
    "    # Get the base64 string\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Create image summaries\n",
    "def image_summarize(img_base64, prompt):\n",
    "    chat = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval.\n",
    "                Remember these images could potentially contain graphs, charts or tables also.\n",
    "                These summaries will be embedded and used to retrieve the raw image for question answering.\n",
    "                Give a detailed summary of the image that is well optimized for retrieval.\n",
    "                Do not add additional words like Summary: etc.\n",
    "             \"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "    return img_base64_list, image_summaries\n",
    "\n",
    "# Image summaries\n",
    "IMG_PATH = './figures'\n",
    "imgs_base64, image_summaries = generate_img_summaries(IMG_PATH)\n",
    "\n",
    "len(imgs_base64), len(image_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c0b1ca-928c-4c1f-972b-7989eb7fb1b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.4. Examine Image Summaries (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00647118-a0a0-4ba0-a93c-a1aaa9f26ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image('./figures/figure-1-1.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d74551-49e6-4ff9-9c30-782349d2d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_summaries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778eeefa-c1e8-46cb-9dc7-ee44806d3e7f",
   "metadata": {},
   "source": [
    "# 4. Build Multi-Vetor Retrievers: Multimodal and Single-Modal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e39e066-8a21-4f57-b69c-744c6df178a4",
   "metadata": {},
   "source": [
    "## 4.1. Access Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59da4116-e124-416b-ab64-a6755c1f53b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af586081-1cf4-4af2-a58a-d6649d0df864",
   "metadata": {},
   "source": [
    "## 4.2. Create Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e35d6f-8488-46b6-a12b-31d2cc481dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_community.storage import RedisStore\n",
    "from langchain_community.utilities.redis import get_client\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Create retriever that indexes summaries, but returns raw images or texts\n",
    "def create_multi_vector_retriever(\n",
    "    docstore, vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
    "):\n",
    "\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=docstore,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    # Check that text_summaries is not empty before adding\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    # Check that table_summaries is not empty before adding\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    # Check that image_summaries is not empty before adding\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e24536-bcfa-4d67-a84a-2566073cd7d3",
   "metadata": {},
   "source": [
    "## 4.3. Initiate Vectorstores: Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71a308a-4ed8-4a76-be3f-ab836b6531dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the summaries and their embeddings\n",
    "chroma_db_multimodal = Chroma(\n",
    "    collection_name=\"mm_rag\",\n",
    "    embedding_function=openai_embed_model,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    ")\n",
    "\n",
    "# The vectorstore for single-modal RAG\n",
    "chroma_db_single_modal = Chroma(\n",
    "    collection_name=\"text_rag\",\n",
    "    embedding_function=openai_embed_model,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6fbd0-af2c-4b83-bd03-941cd7c9dbdb",
   "metadata": {},
   "source": [
    "## 4.4. Initiate Docstores: Redis and InMemoryStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf8baf3-cd0f-4478-aa0d-3a4ee1631e4b",
   "metadata": {},
   "source": [
    "In JupyterLab's terminal (File > New > Terminal), run the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43766181-4b63-45a6-8cb2-7035cb82d1db",
   "metadata": {},
   "source": [
    "```bash\n",
    "# 1. Import the GPG key for the Redis repository\n",
    "curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\n",
    "\n",
    "# 2. Add the Redis repository to your sources list\n",
    "echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] \\\n",
    "https://packages.redis.io/deb $(lsb_release -cs) main\" | \\\n",
    "sudo tee /etc/apt/sources.list.d/redis.list\n",
    "\n",
    "# 3. Update package lists\n",
    "sudo apt-get update\n",
    "\n",
    "# 4. Install Redis Stack Server\n",
    "sudo apt-get install redis-stack-server\n",
    "\n",
    "# 5. Start Redis Stack Server in the background\n",
    "redis-stack-server --daemonize yes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0672eb44-26d8-47e8-9906-892d0b17931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the storage layer - to store raw images, text and tables\n",
    "client = get_client('redis://localhost:6379')\n",
    "redis_store = RedisStore(client=client) # Can use filestore, memorystory, any other DB store also\n",
    "\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "# Initialize the storage layer for the single-modal retriever\n",
    "docstore_single_modal = InMemoryStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34454db-bc8c-47eb-8b71-3e83e309b3f6",
   "metadata": {},
   "source": [
    "## 4.5. Create Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65bd77a-e818-488a-9fea-cde6222cb403",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_multimodal = create_multi_vector_retriever(\n",
    "    redis_store,\n",
    "    chroma_db_multimodal,\n",
    "    text_summaries,\n",
    "    text_docs,\n",
    "    table_summaries,\n",
    "    table_docs,\n",
    "    image_summaries,\n",
    "    imgs_base64,\n",
    ")\n",
    "\n",
    "retriever_multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4fbf15-07de-4af1-bd24-614389c7b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_single_modal = create_multi_vector_retriever(\n",
    "    docstore_single_modal,\n",
    "    chroma_db_single_modal,\n",
    "    text_summaries,\n",
    "    text_docs,\n",
    "    table_summaries=[],  # Empty lists since we're only using text\n",
    "    tables=[],\n",
    "    image_summaries=[],  # No images\n",
    "    images=[],\n",
    ")\n",
    "\n",
    "# Verify the single-modal retriever\n",
    "retriever_single_modal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7172e15b-a782-4bc9-b59c-20a9098b293e",
   "metadata": {},
   "source": [
    "# 5. Prepare for Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb48b26f-4724-4573-88e8-85c0d17d3f0e",
   "metadata": {},
   "source": [
    "## 5.1. Setup Retrieval Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffecf2d-487e-4919-8520-5a6d92576094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display, Markdown, Image\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# Disply base64 encoded string as image\n",
    "def plt_img_base64(img_base64):\n",
    "    # Decode the base64 string\n",
    "    img_data = base64.b64decode(img_base64)\n",
    "    # Create a BytesIO object\n",
    "    img_buffer = BytesIO(img_data)\n",
    "    # Open the image using PIL\n",
    "    img = Image.open(img_buffer)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4e9fe-306a-45f1-bdfe-2d7a11184e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import base64\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xff\\xd8\\xff\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Get the content\n",
    "        if isinstance(doc, Document):\n",
    "            doc_content = doc.page_content\n",
    "        else:\n",
    "            doc_content = doc\n",
    "        # Ensure doc_content is a string\n",
    "        if isinstance(doc_content, bytes):\n",
    "            doc_str = doc_content.decode('utf-8')\n",
    "        else:\n",
    "            doc_str = doc_content  # already a string\n",
    "        if looks_like_base64(doc_str) and is_image_data(doc_str):\n",
    "            b64_images.append(doc_str)\n",
    "        else:\n",
    "            texts.append(doc_str)\n",
    "    return {\"images\": b64_images, \"texts\": texts}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79ab4f9-560d-46e5-a811-6c296227fedc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5.2. Examine Retrievals (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5acecd4-9951-494a-9868-d4623e7e1e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check retrieval\n",
    "query = \"Tell me detailed statistics of the top 5 years with largest wildfire acres burned\"\n",
    "docs = retriever_multimodal.invoke(query, limit=5)\n",
    "\n",
    "# We get 4 docs\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7b0dfc-98ae-419f-a6d0-7fd898ce5671",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8773bc84-4920-4f06-a9fe-b4d7fb622769",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_image_data(docs[1].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b4aae8-3426-491e-8691-9bfc15283741",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = split_image_text_types(docs)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a94141c-fa59-4971-8794-747bdfef8081",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_img_base64(docs[1].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f6d421-511d-4e2c-8d95-9ebb5f70c910",
   "metadata": {},
   "source": [
    "# 6. Construct End-to-End RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e327490-a49a-48bf-af23-1048dd289719",
   "metadata": {},
   "source": [
    "## 6.1. Chain RAG Components Altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6dc7a5-8463-4b7c-913e-4745331ad49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def multimodal_prompt_function(data_dict):\n",
    "    \"\"\"\n",
    "    Create a multimodal prompt with both text and image context.\n",
    "\n",
    "    This function formats the provided context from `data_dict`, which contains\n",
    "    text, tables, and base64-encoded images. It joins the text (with table) portions\n",
    "    and prepares the image(s) in a base64-encoded format to be included in a message.\n",
    "\n",
    "    The formatted text and images (context) along with the user question are used to\n",
    "    construct a prompt for GPT-4o\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            f\"\"\"You are an analyst tasked with understanding detailed information and trends from text documents,\n",
    "                data tables, and charts and graphs in images.\n",
    "                You will be given context information below which will be a mix of text, tables, and images usually of charts or graphs.\n",
    "                Use this information to provide answers related to the user question.\n",
    "                Do not make up answers, use the provided context documents below and answer the question to the best of your ability.\n",
    "\n",
    "                User question:\n",
    "                {data_dict['question']}\n",
    "\n",
    "                Context documents:\n",
    "                {formatted_texts}\n",
    "\n",
    "                Answer:\n",
    "            \"\"\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be88c69-d438-4f27-a99c-c47012af479f",
   "metadata": {},
   "source": [
    "## 6.2. Implement RAG for QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd5f6f3-0d60-4e76-a6b4-ad014d605ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG chain\n",
    "rag = (\n",
    "        {\n",
    "            \"context\": itemgetter('context'),\n",
    "            \"question\": itemgetter('input'),\n",
    "        }\n",
    "            |\n",
    "        RunnableLambda(multimodal_prompt_function)\n",
    "            |\n",
    "        chatgpt\n",
    "            |\n",
    "        StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dea60d-2853-487c-9107-6f495d817f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_qa(query, retriever):\n",
    "\n",
    "    # Pass input query to retriever and get context document elements\n",
    "    retrieve_docs = (itemgetter('input')\n",
    "                        |\n",
    "                    retriever\n",
    "                        |\n",
    "                    RunnableLambda(split_image_text_types))\n",
    "    \n",
    "    # Below, we chain `.assign` calls. This takes a dict and successively\n",
    "    # adds keys-- \"context\" and \"answer\"-- where the value for each key\n",
    "    # is determined by a Runnable (function or chain executing at runtime).\n",
    "    # This helps in also having the retrieved context along with the answer generated by GPT-4o\n",
    "    rag_w_sources = (RunnablePassthrough.assign(context=retrieve_docs)\n",
    "                                        .assign(answer=rag)\n",
    "    )\n",
    "\n",
    "    response = rag_w_sources.invoke({'input': query})\n",
    "    print('=='*50)\n",
    "    print('Answer:')\n",
    "    display(Markdown(response['answer']))\n",
    "    print('--'*50)\n",
    "    print('Sources:')\n",
    "    text_sources = response['context']['texts']\n",
    "    img_sources = response['context']['images']\n",
    "    for text in text_sources:\n",
    "        display(Markdown(text))\n",
    "        print()\n",
    "    for img in img_sources:\n",
    "        plt_img_base64(img)\n",
    "        print()\n",
    "    print('=='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3861b9e-94e2-4787-817d-0a6f78b82a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_rag_qa(query):\n",
    "    # Create a prompt that only includes the user's question\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"You are an assistant that answers questions based solely on the input provided.\\n\\nUser question:\\n{query}\\n\\nAnswer:\")\n",
    "    ]\n",
    "    \n",
    "    response = chatgpt.invoke(messages)\n",
    "    print('=='*50)\n",
    "    print('Answer:')\n",
    "    display(Markdown(response.content))\n",
    "    print('=='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c7b8aa-3b27-4e10-869f-73963ded7557",
   "metadata": {},
   "source": [
    "## 6.3. Examine Multimodal RAG (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341db7a3-fa2e-48e0-a1a1-cdc8b2fad990",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the annual wildfires trend with acres burned\"\n",
    "rag_qa(query, retriever_multimodal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0808f1dc-00d6-4b5c-880e-322427861755",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the number of acres burned by wildfires for the forest service in 2021\"\n",
    "rag_qa(query, retriever_multimodal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3395fcab-0cdf-4bbb-a6f4-1d215d3352c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the percentage of residences burned by wildfires in 2022\"\n",
    "rag_qa(query, retriever_multimodal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70579008-4298-40ca-be1b-628f2d0b576a",
   "metadata": {},
   "source": [
    "## 6.4. Examine Single-Modal RAG (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e190562-8255-4903-92b7-eb05ba4460c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the annual wildfires trend with acres burned\"\n",
    "rag_qa(query, retriever_single_modal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45b6949-d594-470a-867f-c1937b2f518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the number of acres burned by wildfires for the forest service in 2021\"\n",
    "rag_qa(query, retriever_single_modal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ba86f-8d35-4761-89d2-c7d880622366",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the percentage of residences burned by wildfires in 2022\"\n",
    "rag_qa(query, retriever_single_modal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632827b5-52e3-4cd4-81c1-4982be2e761b",
   "metadata": {},
   "source": [
    "## 6.5. Examine No-RAG (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d384f-a844-4b06-b9b1-d7cb84d255b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the annual wildfires trend with acres burned\"\n",
    "no_rag_qa(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3bd382-ec9c-4090-a464-10a2a3db0f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the number of acres burned by wildfires for the forest service in 2021\"\n",
    "no_rag_qa(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818116e6-8a88-4fbb-bed2-7887a8d08929",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the percentage of residences burned by wildfires in 2022\"\n",
    "no_rag_qa(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
