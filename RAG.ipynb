{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3e470b-8da2-41e9-b79b-2e953b486970",
   "metadata": {},
   "source": [
    "This Jupyter Notebook builds upon the referenced [Google Colab](https://colab.research.google.com/drive/1QzQYQxH052Rs088rxMQDveRjoVdkdl74?usp=sharing), integrating customized modifications to address the specific requirements of our testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a3f7ca-2d0e-41a1-9a1b-6c67a48c37c3",
   "metadata": {},
   "source": [
    "# 1. Load Multimodal Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84563197-f172-49a1-a347-d9e459694fff",
   "metadata": {},
   "source": [
    "## 1.1. Extract and Partition Text, Tables, and Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b40a1a8-178d-4f09-99d8-00adb764f249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove existing figures\n",
    "!rm -rf ./figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5bde7b-c7a8-49c6-9311-54c216fd6646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "\n",
    "# Directory containing the PDFs\n",
    "pdf_dir = './references'\n",
    "\n",
    "# Collect all PDF files from the directory and subdirectories, excluding hidden ones\n",
    "pdf_files = []\n",
    "for root, dirs, files in os.walk(pdf_dir):\n",
    "    # Exclude hidden directories\n",
    "    dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "    for file in files:\n",
    "        # Exclude hidden files\n",
    "        if file.lower().endswith('.pdf') and not file.startswith('.'):\n",
    "            pdf_files.append(os.path.join(root, file))\n",
    "\n",
    "# Initialize an empty list to hold data from all PDFs\n",
    "data = []\n",
    "\n",
    "# Loop through each PDF file and load it\n",
    "for pdf_file in pdf_files:\n",
    "    print(f'Loading {pdf_file}')\n",
    "    loader = UnstructuredPDFLoader(\n",
    "        file_path=pdf_file,\n",
    "        strategy='hi_res',\n",
    "        extract_images_in_pdf=True,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",     # section-based chunking\n",
    "        max_characters=4000,              # max size of chunks\n",
    "        new_after_n_chars=4000,           # preferred size of chunks\n",
    "        combine_text_under_n_chars=2000,  # combine smaller chunks\n",
    "        mode='elements',\n",
    "        image_output_dir_path='./figures'\n",
    "    )\n",
    "    data.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd705d3-bc74-4d32-bd0d-2c86d896a6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "tables = []\n",
    "\n",
    "for doc in data:\n",
    "    if doc.metadata['category'] == 'Table':\n",
    "        tables.append(doc)\n",
    "    elif doc.metadata['category'] == 'CompositeElement':\n",
    "        docs.append(doc)\n",
    "\n",
    "len(docs), len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c31a6b-e383-4756-b465-94cc673e7a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import htmltabletomd\n",
    "\n",
    "for table in tables:\n",
    "    table.page_content = htmltabletomd.convert_table(table.metadata['text_as_html'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b525503d-85fb-4416-8d05-898b73822274",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.2. Examine Loaded Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf0d95-a861-48eb-b87b-e61bc2308a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display, Markdown, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e8d372-6a8b-4891-9ef6-7f5b7e715f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97df5c40-bc2b-4ea3-ad03-de53640d5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tables[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ea936-e100-4fc2-8d61-2f7054117691",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables[0].metadata['text_as_html']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d113d-aae5-4d63-8095-c25879911ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(tables[0].metadata['text_as_html']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a59c9a-932f-4aa7-8ee9-737824f36dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in tables:\n",
    "    print(table.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b485e78-3f56-4a14-acbf-2dfe7f4cabf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -l ./figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72b4ef-f8f9-43fa-8c2e-39241342a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./figures/figure-5-4.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ca0769-a9e5-4cf0-b4fb-1923155fb912",
   "metadata": {},
   "source": [
    "# 2. Connect to LLM through API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b662d-ce16-4b2a-a2e4-6ae9b994cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "OPENAI_KEY = getpass('Enter Open AI API Key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9830471c-d740-482a-8cca-e0c3d83bde73",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f0b546-2e3d-4ed6-a275-9e53573ef4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatgpt = ChatOpenAI(model_name='gpt-4o', temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5eebd2-9e7b-4eb4-810a-926241aafc7b",
   "metadata": {},
   "source": [
    "# 3. Generate Text-Based Summaries for Multimodal Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08edce5-36ea-489e-ac74-9dad717f5454",
   "metadata": {},
   "source": [
    "## 3.1. Create Text and Table Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7462704f-5c1d-4ac5-8cc7-7f4ad37d5aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt\n",
    "prompt_text = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text particularly for semantic retrieval.\n",
    "These summaries will be embedded and used to retrieve the raw text or table elements.\n",
    "Give a detailed summary of the table or text below that is well optimized for retrieval.\n",
    "For any tables also add in a one line description of what the table is about besides the summary.\n",
    "Do not add additional words like Summary: etc.\n",
    "\n",
    "Table or text chunk:\n",
    "{element}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Summary chain\n",
    "summarize_chain = (\n",
    "                    {\"element\": RunnablePassthrough()}\n",
    "                      |\n",
    "                    prompt\n",
    "                      |\n",
    "                    chatgpt\n",
    "                      |\n",
    "                    StrOutputParser() # Extracts the response as text and returns it as a string\n",
    ")\n",
    "\n",
    "# Initialize empty summaries\n",
    "text_summaries = []\n",
    "table_summaries = []\n",
    "\n",
    "text_docs = [doc.page_content for doc in docs]\n",
    "table_docs = [table.page_content for table in tables]\n",
    "\n",
    "text_summaries = summarize_chain.batch(text_docs, {\"max_concurrency\": 5})\n",
    "table_summaries = summarize_chain.batch(table_docs, {\"max_concurrency\": 5})\n",
    "\n",
    "len(text_summaries), len(table_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f24782-8538-4abb-96e8-dbc2e14f5418",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.2. Examine Text and Table Summaries (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38a1e4f-e5ea-4258-9eed-bec50ef3d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18840116-269a-419f-ad75-a574ba00e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_summaries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd91cd0-bcf5-476c-a949-554757c38f66",
   "metadata": {},
   "source": [
    "## 3.3. Create Image Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97e35e8-e966-453d-8ba5-80fe57b7f001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a function to encode images\n",
    "def encode_image(image_path):\n",
    "    # Get the base64 string\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Create image summaries\n",
    "def image_summarize(img_base64, prompt):\n",
    "    chat = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval.\n",
    "                Remember these images could potentially contain graphs, charts or tables also.\n",
    "                These summaries will be embedded and used to retrieve the raw image for question answering.\n",
    "                Give a detailed summary of the image that is well optimized for retrieval.\n",
    "                Do not add additional words like Summary: etc.\n",
    "             \"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "    return img_base64_list, image_summaries\n",
    "\n",
    "# Image summaries\n",
    "IMG_PATH = './figures'\n",
    "imgs_base64, image_summaries = generate_img_summaries(IMG_PATH)\n",
    "\n",
    "len(imgs_base64), len(image_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c0b1ca-928c-4c1f-972b-7989eb7fb1b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.4. Examine Image Summaries (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00647118-a0a0-4ba0-a93c-a1aaa9f26ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image('./figures/figure-1-1.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d74551-49e6-4ff9-9c30-782349d2d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_summaries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778eeefa-c1e8-46cb-9dc7-ee44806d3e7f",
   "metadata": {},
   "source": [
    "# 4. Build Multi-Vetor Retrievers: Multimodal and Single-Modal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e39e066-8a21-4f57-b69c-744c6df178a4",
   "metadata": {},
   "source": [
    "## 4.1. Access Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59da4116-e124-416b-ab64-a6755c1f53b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af586081-1cf4-4af2-a58a-d6649d0df864",
   "metadata": {},
   "source": [
    "## 4.2. Create Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e35d6f-8488-46b6-a12b-31d2cc481dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_community.storage import RedisStore\n",
    "from langchain_community.utilities.redis import get_client\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Create retriever that indexes summaries, but returns raw images or texts\n",
    "def create_multi_vector_retriever(\n",
    "    docstore, vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
    "):\n",
    "\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=docstore,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    # Check that text_summaries is not empty before adding\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    # Check that table_summaries is not empty before adding\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    # Check that image_summaries is not empty before adding\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e24536-bcfa-4d67-a84a-2566073cd7d3",
   "metadata": {},
   "source": [
    "## 4.3. Initiate Vectorstores: Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71a308a-4ed8-4a76-be3f-ab836b6531dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the summaries and their embeddings\n",
    "chroma_db_multimodal = Chroma(\n",
    "    collection_name=\"mm_rag\",\n",
    "    embedding_function=openai_embed_model,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    ")\n",
    "\n",
    "# The vectorstore for single-modal RAG\n",
    "chroma_db_single_modal = Chroma(\n",
    "    collection_name=\"text_rag\",\n",
    "    embedding_function=openai_embed_model,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6fbd0-af2c-4b83-bd03-941cd7c9dbdb",
   "metadata": {},
   "source": [
    "## 4.4. Initiate Docstores: Redis and InMemoryStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf8baf3-cd0f-4478-aa0d-3a4ee1631e4b",
   "metadata": {},
   "source": [
    "In JupyterLab's terminal (File > New > Terminal), run the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43766181-4b63-45a6-8cb2-7035cb82d1db",
   "metadata": {},
   "source": [
    "```bash\n",
    "# 1. Import the GPG key for the Redis repository\n",
    "curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\n",
    "\n",
    "# 2. Add the Redis repository to your sources list\n",
    "echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] \\\n",
    "https://packages.redis.io/deb $(lsb_release -cs) main\" | \\\n",
    "sudo tee /etc/apt/sources.list.d/redis.list\n",
    "\n",
    "# 3. Update package lists\n",
    "sudo apt-get update\n",
    "\n",
    "# 4. Install Redis Stack Server\n",
    "sudo apt-get install redis-stack-server\n",
    "\n",
    "# 5. Start Redis Stack Server in the background\n",
    "redis-stack-server --daemonize yes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0672eb44-26d8-47e8-9906-892d0b17931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the storage layer - to store raw images, text and tables\n",
    "client = get_client('redis://localhost:6379')\n",
    "redis_store = RedisStore(client=client) # Can use filestore, memorystory, any other DB store also\n",
    "\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "# Initialize the storage layer for the single-modal retriever\n",
    "docstore_single_modal = InMemoryStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34454db-bc8c-47eb-8b71-3e83e309b3f6",
   "metadata": {},
   "source": [
    "## 4.5. Create Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65bd77a-e818-488a-9fea-cde6222cb403",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_multimodal = create_multi_vector_retriever(\n",
    "    redis_store,\n",
    "    chroma_db_multimodal,\n",
    "    text_summaries,\n",
    "    text_docs,\n",
    "    table_summaries,\n",
    "    table_docs,\n",
    "    image_summaries,\n",
    "    imgs_base64,\n",
    ")\n",
    "\n",
    "retriever_multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4fbf15-07de-4af1-bd24-614389c7b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_single_modal = create_multi_vector_retriever(\n",
    "    docstore_single_modal,\n",
    "    chroma_db_single_modal,\n",
    "    text_summaries,\n",
    "    text_docs,\n",
    "    table_summaries=[],  # Empty lists since we're only using text\n",
    "    tables=[],\n",
    "    image_summaries=[],  # No images\n",
    "    images=[],\n",
    ")\n",
    "\n",
    "# Verify the single-modal retriever\n",
    "retriever_single_modal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7172e15b-a782-4bc9-b59c-20a9098b293e",
   "metadata": {},
   "source": [
    "# 5. Prepare for Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb48b26f-4724-4573-88e8-85c0d17d3f0e",
   "metadata": {},
   "source": [
    "## 5.1. Setup Retrieval Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffecf2d-487e-4919-8520-5a6d92576094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display, Markdown, Image\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# Disply base64 encoded string as image\n",
    "def plt_img_base64(img_base64):\n",
    "    # Decode the base64 string\n",
    "    img_data = base64.b64decode(img_base64)\n",
    "    # Create a BytesIO object\n",
    "    img_buffer = BytesIO(img_data)\n",
    "    # Open the image using PIL\n",
    "    img = Image.open(img_buffer)\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4e9fe-306a-45f1-bdfe-2d7a11184e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import base64\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xff\\xd8\\xff\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Get the content\n",
    "        if isinstance(doc, Document):\n",
    "            doc_content = doc.page_content\n",
    "        else:\n",
    "            doc_content = doc\n",
    "        # Ensure doc_content is a string\n",
    "        if isinstance(doc_content, bytes):\n",
    "            doc_str = doc_content.decode('utf-8')\n",
    "        else:\n",
    "            doc_str = doc_content  # already a string\n",
    "        if looks_like_base64(doc_str) and is_image_data(doc_str):\n",
    "            b64_images.append(doc_str)\n",
    "        else:\n",
    "            texts.append(doc_str)\n",
    "    return {\"images\": b64_images, \"texts\": texts}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79ab4f9-560d-46e5-a811-6c296227fedc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5.2. Examine Retrievals (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5acecd4-9951-494a-9868-d4623e7e1e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check retrieval\n",
    "query = \"Tell me detailed statistics of the top 5 years with largest wildfire acres burned\"\n",
    "docs = retriever_multimodal.invoke(query, limit=5)\n",
    "\n",
    "# We get 4 docs\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7b0dfc-98ae-419f-a6d0-7fd898ce5671",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8773bc84-4920-4f06-a9fe-b4d7fb622769",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_image_data(docs[1].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b4aae8-3426-491e-8691-9bfc15283741",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = split_image_text_types(docs)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a94141c-fa59-4971-8794-747bdfef8081",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_img_base64(docs[1].decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f6d421-511d-4e2c-8d95-9ebb5f70c910",
   "metadata": {},
   "source": [
    "# 6. Construct End-to-End RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e327490-a49a-48bf-af23-1048dd289719",
   "metadata": {},
   "source": [
    "## 6.1. Chain RAG Components Altogether"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6dc7a5-8463-4b7c-913e-4745331ad49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def multimodal_prompt_function(data_dict):\n",
    "    \"\"\"\n",
    "    Create a multimodal prompt with both text and image context.\n",
    "\n",
    "    This function formats the provided context from `data_dict`, which contains\n",
    "    text, tables, and base64-encoded images. It joins the text (with table) portions\n",
    "    and prepares the image(s) in a base64-encoded format to be included in a message.\n",
    "\n",
    "    The formatted text and images (context) along with the user question are used to\n",
    "    construct a prompt for GPT-4o\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            f\"\"\"You are an analyst tasked with understanding detailed information and trends from text documents,\n",
    "                data tables, and charts and graphs in images.\n",
    "                You will be given context information below which will be a mix of text, tables, and images usually of charts or graphs.\n",
    "                Use this information to provide answers related to the user question.\n",
    "                Do not make up answers, use the provided context documents below and answer the question to the best of your ability.\n",
    "\n",
    "                User question:\n",
    "                {data_dict['question']}\n",
    "\n",
    "                Context documents:\n",
    "                {formatted_texts}\n",
    "\n",
    "                Answer:\n",
    "            \"\"\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be88c69-d438-4f27-a99c-c47012af479f",
   "metadata": {},
   "source": [
    "## 6.2. Implement RAG for QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd5f6f3-0d60-4e76-a6b4-ad014d605ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG chain\n",
    "rag = (\n",
    "        {\n",
    "            \"context\": itemgetter('context'),\n",
    "            \"question\": itemgetter('input'),\n",
    "        }\n",
    "            |\n",
    "        RunnableLambda(multimodal_prompt_function)\n",
    "            |\n",
    "        chatgpt\n",
    "            |\n",
    "        StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dea60d-2853-487c-9107-6f495d817f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_qa(query, retriever):\n",
    "\n",
    "    # Pass input query to retriever and get context document elements\n",
    "    retrieve_docs = (itemgetter('input')\n",
    "                        |\n",
    "                    retriever\n",
    "                        |\n",
    "                    RunnableLambda(split_image_text_types))\n",
    "    \n",
    "    # Below, we chain `.assign` calls. This takes a dict and successively\n",
    "    # adds keys-- \"context\" and \"answer\"-- where the value for each key\n",
    "    # is determined by a Runnable (function or chain executing at runtime).\n",
    "    # This helps in also having the retrieved context along with the answer generated by GPT-4o\n",
    "    rag_w_sources = (RunnablePassthrough.assign(context=retrieve_docs)\n",
    "                                        .assign(answer=rag)\n",
    "    )\n",
    "\n",
    "    response = rag_w_sources.invoke({'input': query})\n",
    "    print('=='*50)\n",
    "    print('Answer:')\n",
    "    display(Markdown(response['answer']))\n",
    "    print('--'*50)\n",
    "    print('Sources:')\n",
    "    text_sources = response['context']['texts']\n",
    "    img_sources = response['context']['images']\n",
    "    for text in text_sources:\n",
    "        display(Markdown(text))\n",
    "        print()\n",
    "    for img in img_sources:\n",
    "        plt_img_base64(img)\n",
    "        print()\n",
    "    print('=='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3861b9e-94e2-4787-817d-0a6f78b82a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_rag_qa(query):\n",
    "    # Create a prompt that only includes the user's question\n",
    "    messages = [\n",
    "        HumanMessage(content=f\"You are an assistant that answers questions based solely on the input provided.\\n\\nUser question:\\n{query}\\n\\nAnswer:\")\n",
    "    ]\n",
    "    \n",
    "    response = chatgpt.invoke(messages)\n",
    "    print('=='*50)\n",
    "    print('Answer:')\n",
    "    display(Markdown(response.content))\n",
    "    print('=='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c7b8aa-3b27-4e10-869f-73963ded7557",
   "metadata": {},
   "source": [
    "## 6.3. Examine Multimodal RAG (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341db7a3-fa2e-48e0-a1a1-cdc8b2fad990",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the annual wildfires trend with acres burned\"\n",
    "rag_qa(query, retriever_multimodal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0808f1dc-00d6-4b5c-880e-322427861755",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the number of acres burned by wildfires for the forest service in 2021\"\n",
    "rag_qa(query, retriever_multimodal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3395fcab-0cdf-4bbb-a6f4-1d215d3352c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the percentage of residences burned by wildfires in 2022\"\n",
    "rag_qa(query, retriever_multimodal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70579008-4298-40ca-be1b-628f2d0b576a",
   "metadata": {},
   "source": [
    "## 6.4. Examine Single-Modal RAG (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e190562-8255-4903-92b7-eb05ba4460c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the annual wildfires trend with acres burned\"\n",
    "rag_qa(query, retriever_single_modal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45b6949-d594-470a-867f-c1937b2f518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the number of acres burned by wildfires for the forest service in 2021\"\n",
    "rag_qa(query, retriever_single_modal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ba86f-8d35-4761-89d2-c7d880622366",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the percentage of residences burned by wildfires in 2022\"\n",
    "rag_qa(query, retriever_single_modal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632827b5-52e3-4cd4-81c1-4982be2e761b",
   "metadata": {},
   "source": [
    "## 6.5. Examine No-RAG (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d384f-a844-4b06-b9b1-d7cb84d255b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the annual wildfires trend with acres burned\"\n",
    "no_rag_qa(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3bd382-ec9c-4090-a464-10a2a3db0f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the number of acres burned by wildfires for the forest service in 2021\"\n",
    "no_rag_qa(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818116e6-8a88-4fbb-bed2-7887a8d08929",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about the percentage of residences burned by wildfires in 2022\"\n",
    "no_rag_qa(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc6d40d-aec4-437c-8169-961b18a14ce5",
   "metadata": {},
   "source": [
    "# 7. Query Energy Advisor Exam Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb2487-c043-4216-a0ca-b36ac020486c",
   "metadata": {},
   "source": [
    "## 7.1. Read the Excel File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768f900c-037a-46ed-9617-89d83789a75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import base64\n",
    "import os\n",
    "from io import BytesIO\n",
    "from PIL import Image as PILImage\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# Path to the Excel file\n",
    "excel_file_path = './EA QAs/EA QAs.xlsx'\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "questions_df = pd.read_excel(excel_file_path, sheet_name='EA QAs')\n",
    "\n",
    "# Ensure the columns are correct\n",
    "print(questions_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4f4e75-07e2-464c-98fa-b30522cd73cd",
   "metadata": {},
   "source": [
    "## 7.2. Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff01a02-656c-4f5b-bc3f-4e8d081529aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text):\n",
    "    \"\"\"Extracts the agent's answer, ensuring it is one of (A), (B), (C), or (D).\"\"\"\n",
    "    match = re.search(r'\\b([ABCD])\\b', text.strip().upper())\n",
    "    return match.group(1) if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d4ad93-1551-43e6-9fd0-fed009b2e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path, max_size=(800, 800)):\n",
    "    \"\"\"\n",
    "    Encodes an image file to a Base64 string after resizing it to a maximum size.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with PILImage.open(image_path) as img:\n",
    "            # Resize the image while maintaining aspect ratio\n",
    "            img.thumbnail(max_size, PILImage.LANCZOS)\n",
    "            \n",
    "            # Save the image to a BytesIO object\n",
    "            buffered = BytesIO()\n",
    "            img.save(buffered, format=\"JPEG\", quality=85)\n",
    "            \n",
    "            # Encode the image to Base64\n",
    "            img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "            return img_str\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Image file {image_path} not found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e600527-6ac5-490f-ab24-b1d380d396f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_base64_image(img_base64, max_size=(800, 800)):\n",
    "    \"\"\"Resizes a Base64-encoded image to the specified maximum size.\"\"\"\n",
    "    img_data = base64.b64decode(img_base64)\n",
    "    img = PILImage.open(BytesIO(img_data))\n",
    "    img.thumbnail(max_size, PILImage.LANCZOS)\n",
    "    buffered = BytesIO()\n",
    "    img.save(buffered, format=\"JPEG\", quality=85)\n",
    "    resized_img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "    return resized_img_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0056fb8-e3f9-4709-9d6b-e69d4e8fd85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_docs_into_images_texts_tables(docs):\n",
    "    \"\"\"Splits documents into images, texts, and tables.\"\"\"\n",
    "    images = []\n",
    "    texts = []\n",
    "    tables = []\n",
    "    for doc in docs:\n",
    "        # Extract content and metadata\n",
    "        content = doc.page_content if hasattr(doc, 'page_content') else str(doc)\n",
    "        \n",
    "        # Decode content if it's bytes\n",
    "        if isinstance(content, bytes):\n",
    "            content = content.decode('utf-8', errors='ignore')\n",
    "        \n",
    "        metadata = doc.metadata if hasattr(doc, 'metadata') else {}\n",
    "        category = metadata.get('category', '').lower()\n",
    "        \n",
    "        # Check the category to classify the content\n",
    "        if content.startswith('data:image'):\n",
    "            images.append(content)\n",
    "        elif category == 'table' or '<table' in content.lower():\n",
    "            tables.append({'content': content, 'metadata': metadata})\n",
    "        else:\n",
    "            texts.append(content)\n",
    "    return {'images': images, 'texts': texts, 'tables': tables}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80402a75-74b2-47e0-82f5-78f0d4db99ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_base64_image(img_base64):\n",
    "    \"\"\"Displays a Base64-encoded image.\"\"\"\n",
    "    img_data = base64.b64decode(img_base64)\n",
    "    img = PILImage.open(BytesIO(img_data))\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecbe94a-4ccb-4315-b5e8-e2bfc484ff7d",
   "metadata": {},
   "source": [
    "## 7.3. Modify RAG Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c896d9-0721-45b3-ab34-f20ff4a9b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_multiple_choice_qa(question, options, retriever, image_base64=None):\n",
    "    \"\"\"Answers a multiple-choice question using the given retriever.\"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = retriever.get_relevant_documents(question)\n",
    "    \n",
    "    # Split documents into images, texts, and tables\n",
    "    sources = split_docs_into_images_texts_tables(retrieved_docs)\n",
    "    \n",
    "    # Limit text sources to 100 words\n",
    "    def limit_text_length(text):\n",
    "        words = text.split()\n",
    "        return ' '.join(words[:100])\n",
    "    \n",
    "    sources['texts'] = [limit_text_length(text) for text in sources['texts']]\n",
    "    \n",
    "    # Build the prompt\n",
    "    formatted_texts = \"\\n\".join(sources['texts'])\n",
    "    messages = []\n",
    "    \n",
    "    # Include the question image in the messages if provided\n",
    "    if image_base64:\n",
    "        # Resize the question image if necessary\n",
    "        resized_image_base64 = resize_base64_image(image_base64)\n",
    "        messages.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{resized_image_base64}\"}\n",
    "        })\n",
    "    \n",
    "    # Add images from retrieved sources to the messages\n",
    "    for image_data in sources['images']:\n",
    "        # Extract the Base64 data\n",
    "        img_base64 = image_data.split(',', 1)[1] if ',' in image_data else image_data\n",
    "        # Resize the image\n",
    "        resized_img_base64 = resize_base64_image(img_base64)\n",
    "        # Reconstruct the data URL\n",
    "        resized_image_data = f\"data:image/jpeg;base64,{resized_img_base64}\"\n",
    "        messages.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": resized_image_data},\n",
    "        })\n",
    "    \n",
    "    # Add tables from retrieved sources to the messages\n",
    "    for table_dict in sources['tables']:\n",
    "        table_content = table_dict['content']\n",
    "        # Decode table_content if it's bytes\n",
    "        if isinstance(table_content, bytes):\n",
    "            table_content = table_content.decode('utf-8', errors='ignore')\n",
    "        messages.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": table_content  # Include the table content as text (HTML)\n",
    "        })\n",
    "    \n",
    "    # Add the text message\n",
    "    prompt_text = f\"\"\"You are an assistant that answers multiple-choice questions based solely on the input provided.\n",
    "You will be given a question, several options, and context documents, which may include text, HTML tables, and images.\n",
    "Use the context documents to choose the correct option.\n",
    "Respond with only the letter of the chosen option: (A), (B), (C), or (D).\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\n",
    "Options:\n",
    "{options}\n",
    "\n",
    "Context documents:\n",
    "{formatted_texts}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "    messages.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": prompt_text\n",
    "    })\n",
    "    \n",
    "    # Send the messages to the LLM\n",
    "    response = chatgpt.invoke([HumanMessage(content=messages)])\n",
    "    \n",
    "    # Extract the answer\n",
    "    answer_text = response.content.strip()\n",
    "    answer = extract_answer(answer_text)\n",
    "    \n",
    "    return answer, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a4181-b8b7-4cf7-ac3c-3a9272f6a073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_rag_multiple_choice_qa(question, options, image_base64=None):\n",
    "    \"\"\"Answers a multiple-choice question without retrieval (no RAG).\"\"\"\n",
    "    messages = []\n",
    "    \n",
    "    # Include the question image in the messages if provided\n",
    "    if image_base64:\n",
    "        # Resize the question image if necessary\n",
    "        resized_image_base64 = resize_base64_image(image_base64)\n",
    "        messages.append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{resized_image_base64}\"}\n",
    "        })\n",
    "    \n",
    "    # Add the question and options\n",
    "    prompt_text = f\"\"\"You are an assistant that answers multiple-choice questions based solely on the input provided.\n",
    "You will be given a question and several options.\n",
    "Choose the correct option from the given options, and respond with only the letter of the chosen option: (A), (B), (C), or (D).\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\n",
    "Options:\n",
    "{options}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    messages.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": prompt_text\n",
    "    })\n",
    "    \n",
    "    # Send the messages to the LLM\n",
    "    response = chatgpt.invoke([HumanMessage(content=messages)])\n",
    "    \n",
    "    # Extract the answer\n",
    "    answer_text = response.content.strip()\n",
    "    answer = extract_answer(answer_text)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5892643-3547-4fab-83cb-fe0a4d2408d7",
   "metadata": {},
   "source": [
    "## 7.4. Process Each Question and Collect Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c08725-5d92-4922-b536-d47388e69534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop to process the questions\n",
    "for idx, row in questions_df.iterrows():\n",
    "    question_number = row['#']\n",
    "    question_text = row['Question']\n",
    "    image_needed = row['Image']\n",
    "    options_text = row['Option']\n",
    "    true_answer = row['Answer']\n",
    "\n",
    "    # Load image if needed\n",
    "    image_base64 = None\n",
    "    if str(image_needed).strip().upper() == 'TRUE':\n",
    "        image_path = os.path.join('EA QAs', f\"{question_number}.jpg\")\n",
    "        image_base64 = encode_image(image_path, max_size=(800, 800))  # Adjust size as needed\n",
    "\n",
    "    # Multimodal RAG agent\n",
    "    mm_answer, mm_sources = rag_multiple_choice_qa(\n",
    "        question_text, options_text, retriever_multimodal, image_base64\n",
    "    )\n",
    "\n",
    "    # Single-modal RAG agent (no images in context)\n",
    "    sm_answer, sm_sources = rag_multiple_choice_qa(\n",
    "        question_text, options_text, retriever_single_modal, image_base64=None\n",
    "    )\n",
    "\n",
    "    # No-RAG agent\n",
    "    nr_answer = no_rag_multiple_choice_qa(\n",
    "        question_text, options_text, image_base64\n",
    "    )\n",
    "\n",
    "    # Compare answers to true answer\n",
    "    mm_correct = 'CORRECT' if mm_answer == true_answer else 'WRONG'\n",
    "    sm_correct = 'CORRECT' if sm_answer == true_answer else 'WRONG'\n",
    "    nr_correct = 'CORRECT' if nr_answer == true_answer else 'WRONG'\n",
    "\n",
    "    # Display the results\n",
    "    print(f\"[{question_number}]\")\n",
    "    print(\"-\"*30)\n",
    "    print(f\"({true_answer}): True answer\\n\")\n",
    "    print(f\"({mm_answer}): Multimodal RAG - {mm_correct}\")\n",
    "    print(f\"({sm_answer}): Single-modal RAG - {sm_correct}\")\n",
    "    print(f\"({nr_answer}): No-RAG - {nr_correct}\")\n",
    "    print(\"-\"*30)\n",
    "    print(\"Question:\")\n",
    "    print(question_text)\n",
    "    if image_base64:\n",
    "        display_base64_image(image_base64)\n",
    "    print(\"\\nOptions:\")\n",
    "    print(options_text)\n",
    "\n",
    "    '''\n",
    "    print(\"-\"*30)\n",
    "    print(\"Multimodal RAG:\")\n",
    "    for i, text in enumerate(mm_sources['texts']):\n",
    "        print(f\"Source {i+1}:\")\n",
    "        display(Markdown(text))\n",
    "        print()\n",
    "    for i, img_data in enumerate(mm_sources['images']):\n",
    "        print(f\"Image Source {i+1}:\")\n",
    "        # Extract Base64 data\n",
    "        img_base64 = img_data.split(',', 1)[1] if ',' in img_data else img_data\n",
    "        # Resize the image for display\n",
    "        resized_img_base64 = resize_base64_image(img_base64)\n",
    "        display_base64_image(resized_img_base64)\n",
    "        print()\n",
    "    # Display tables if any\n",
    "    for i, table_dict in enumerate(mm_sources['tables']):\n",
    "        print(f\"Table Source {i+1}:\")\n",
    "        table_content = table_dict['content']\n",
    "        # Decode table_content if it's bytes\n",
    "        if isinstance(table_content, bytes):\n",
    "            table_content = table_content.decode('utf-8', errors='ignore')\n",
    "        display(HTML(table_content))\n",
    "        print()\n",
    "    print(\"-\"*30)\n",
    "    print(\"Single-modal RAG:\")\n",
    "    for i, text in enumerate(sm_sources['texts']):\n",
    "        print(f\"Source {i+1}:\")\n",
    "        display(Markdown(text))\n",
    "        print()\n",
    "    # No images or tables for single-modal RAG\n",
    "    '''\n",
    "    \n",
    "    print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9141b5a8-6df4-4998-b617-570d8291491d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
