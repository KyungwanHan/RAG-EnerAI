{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b70a0b7-1433-4b36-97dc-8cc92a80966e",
   "metadata": {},
   "source": [
    "# Testing Large Language Model Agents with Retrieval-Augmented Generation\n",
    "In this notebook, we'll explore how to test various large language model (LLM) agents powered by Retrieval-Augmented Generation (RAG). We'll walk through the entire process, from loading and preprocessing multimodal data to querying exam questions using different RAG strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025e85c2-f39e-4789-894f-0a79560810e3",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup](#1-setup)\n",
    "2. [Data Loading and Preprocessing](#2-data-loading-and-preprocessing)\n",
    "    - [2.1. Load Multimodal Data](#21-load-multimodal-data)\n",
    "    - [2.2. Extract and Partition Text, Tables, and Images](#22-extract-and-partition-text-tables-and-images)\n",
    "3. [Connecting to the Language Model](#3-connecting-to-the-language-model)\n",
    "4. [Generating Summaries for Multimodal Data](#4-generating-summaries-for-multimodal-data)\n",
    "    - [4.1. Create Text and Table Summaries](#41-create-text-and-table-summaries)\n",
    "    - [4.2. Create Image Summaries](#42-create-image-summaries)\n",
    "5. [Building Vector Retrievers](#5-building-vector-retrievers)\n",
    "    - [5.1. Access Embedding Model](#51-access-embedding-model)\n",
    "    - [5.2. Create Utility Functions](#52-create-utility-functions)\n",
    "    - [5.3. Initiate Vectorstores: Chroma](#53-initiate-vectorstores-chroma)\n",
    "    - [5.4. Initiate Docstores: Redis and InMemoryStore](#54-initiate-docstores-redis-and-inmemorystore)\n",
    "    - [5.5. Create Retrievers](#55-create-retrievers)\n",
    "6. [Querying Exam Questions](#6-querying-exam-questions)\n",
    "    - [6.1. Read the Excel File](#61-read-the-excel-file)\n",
    "    - [6.2. Define Helper Functions](#62-define-helper-functions)\n",
    "    - [6.3. Modify RAG Functions](#63-modify-rag-functions)\n",
    "    - [6.4. Process Each Question and Collect Responses](#64-process-each-question-and-collect-responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7dc6e5-497e-4b79-85fd-68fb3746fd72",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bee7cfe-1a61-450e-ad45-802d033f941c",
   "metadata": {},
   "source": [
    "Before diving into the core functionalities, let's set up our environment by importing the necessary libraries and configuring essential settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc4a7dc-26b1-4228-8df2-524d2307a7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import re\n",
    "import base64\n",
    "import htmltabletomd\n",
    "import logging\n",
    "import requests\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63a4056-dfcf-4590-9205-c49e79b6b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO, StringIO\n",
    "from PIL import Image as PILImage\n",
    "from IPython.display import display, Markdown, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd79bc4-adc5-4d53-9933-b1b670cf48e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain and related libraries\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_community.storage import RedisStore\n",
    "from langchain_community.utilities.redis import get_client\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain.storage import InMemoryStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f69eb61-a32f-4dd4-b16d-60ff00fba133",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db41c8c1-65e9-46c8-8b08-a3b166a16497",
   "metadata": {},
   "source": [
    "In this section, we'll load multimodal data (PDFs containing text, tables, and images) and preprocess it for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3365ed1d-9932-4e93-80a4-ecd7ec4b480f",
   "metadata": {},
   "source": [
    "### 2.1. Load Multimodal Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc6ae0-8638-427e-84b4-b8d433e259e0",
   "metadata": {},
   "source": [
    "We'll start by locating all PDF files within the specified directory and its subdirectories. This setup ensures that we process all relevant documents while excluding hidden files and directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db088c4-1b14-45b7-833a-254738d932fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove existing figures to ensure a clean workspace\n",
    "!rm -rf ./figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab7e40c-4fac-4dba-a2cf-3d2b587ba4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the PDFs\n",
    "pdf_dir = './references'\n",
    "\n",
    "# Collect all PDF files from the directory and subdirectories, excluding hidden ones\n",
    "pdf_files = []\n",
    "\n",
    "for root, dirs, files in os.walk(pdf_dir):\n",
    "    # Exclude hidden directories\n",
    "    dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "    for file in files:\n",
    "        # Exclude hidden files and ensure the file has a .pdf extension\n",
    "        if file.lower().endswith('.pdf') and not file.startswith('.'):\n",
    "            pdf_files.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0e288f-2f57-4e35-9adb-a9e8a432266b",
   "metadata": {},
   "source": [
    "### 2.2. Extract and Partition Text, Tables, and Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6467d3-d7af-4fe3-a86b-88d3c3d4df6c",
   "metadata": {},
   "source": [
    "Next, we'll extract the content from each PDF using UnstructuredPDFLoader. The loader is configured to extract text, tables, and images, and to partition the content into manageable chunks based on titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec4b412-d0d8-4ebc-b677-e735f14d515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold data from all PDFs\n",
    "data = []\n",
    "\n",
    "# Loop through each PDF file and load its content\n",
    "for pdf_file in pdf_files:\n",
    "    print(f'Loading {pdf_file}')\n",
    "    loader = UnstructuredPDFLoader(\n",
    "        file_path=pdf_file,\n",
    "        strategy='hi_res',\n",
    "        extract_images_in_pdf=True,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",     # Section-based chunking\n",
    "        max_characters=4000,              # Max size of chunks\n",
    "        new_after_n_chars=4000,           # Preferred size of chunks\n",
    "        combine_text_under_n_chars=2000,  # Combine smaller chunks\n",
    "        mode='elements',\n",
    "        image_output_dir_path='./figures'\n",
    "    )\n",
    "    data.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358a5eb1-6a0a-462a-9cbb-b571d03cc952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate documents and tables based on metadata\n",
    "docs = []\n",
    "tables = []\n",
    "\n",
    "for doc in data:\n",
    "    if doc.metadata['category'] == 'Table':\n",
    "        tables.append(doc)\n",
    "    elif doc.metadata['category'] == 'CompositeElement':\n",
    "        docs.append(doc)\n",
    "\n",
    "# Display the number of documents and tables extracted\n",
    "len(docs), len(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0942bb-53a5-455f-8188-1bb6bf228f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert HTML tables to Markdown for easier readability and processing\n",
    "for table in tables:\n",
    "    table.page_content = htmltabletomd.convert_table(table.metadata['text_as_html'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f324610d-958f-40ce-b952-0690193f351b",
   "metadata": {},
   "source": [
    "## 3. Connecting to the Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110b0abd-71d2-4ff8-9a60-ab3313f2af0f",
   "metadata": {},
   "source": [
    "To interact with the OpenAI language models, we'll establish a connection using the OpenAI API. You'll be prompted to enter your API key securely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4731312b-e981-4942-8055-c2e9454ee0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "# Prompt the user to enter their OpenAI API Key securely\n",
    "OPENAI_KEY = getpass('Enter Open AI API Key: ')\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afaff46-499b-4f33-b3e7-f31173984f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ChatOpenAI model with desired parameters\n",
    "chatgpt = ChatOpenAI(model_name='gpt-4o', temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34371d8e-7db3-4d1a-bb8f-b8ec2407f61c",
   "metadata": {},
   "source": [
    "## 4. Generating Summaries for Multimodal Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1836ad-5a53-411e-ac27-4a7ce9c3e41a",
   "metadata": {},
   "source": [
    "Summarizing the extracted data is crucial for efficient retrieval. We'll generate summaries for texts, tables, and images to optimize them for semantic retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c39bfcc-d4d2-47e6-ab1a-875d052e1c29",
   "metadata": {},
   "source": [
    "### 4.1. Create Text and Table Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4816a5a-f812-4a93-b9a3-6efbf0973c0d",
   "metadata": {},
   "source": [
    "Using a tailored prompt, we'll instruct the language model to generate detailed summaries of text and tables. These summaries are designed to be easily embedded and retrieved later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa61e54-5f53-4725-9495-c5651e174e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template for summarization\n",
    "prompt_text = \"\"\"\n",
    "You are an assistant tasked with summarizing tables and text particularly for semantic retrieval.\n",
    "These summaries will be embedded and used to retrieve the raw text or table elements.\n",
    "Give a detailed summary of the table or text below that is well optimized for retrieval.\n",
    "For any tables also add in a one line description of what the table is about besides the summary.\n",
    "Do not add additional words like Summary: etc.\n",
    "\n",
    "Table or text chunk:\n",
    "{element}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "# Define the summarization chain\n",
    "summarize_chain = (\n",
    "    {\"element\": RunnablePassthrough()}\n",
    "      |\n",
    "    prompt\n",
    "      |\n",
    "    chatgpt\n",
    "      |\n",
    "    StrOutputParser()  # Extracts the response as text and returns it as a string\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec203caa-411d-46fd-860b-f9ba776178f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to hold summaries\n",
    "text_summaries = []\n",
    "table_summaries = []\n",
    "\n",
    "# Prepare documents for summarization\n",
    "text_docs = [doc.page_content for doc in docs]\n",
    "table_docs = [table.page_content for table in tables]\n",
    "\n",
    "# Generate summaries in batches with concurrency\n",
    "text_summaries = summarize_chain.batch(text_docs, {\"max_concurrency\": 5})\n",
    "table_summaries = summarize_chain.batch(table_docs, {\"max_concurrency\": 5})\n",
    "\n",
    "# Display the number of summaries generated\n",
    "len(text_summaries), len(table_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe7204-9719-4175-8166-e213ca2dd536",
   "metadata": {},
   "source": [
    "### 4.2. Create Image Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a168dfb-345d-41e6-9ad1-02bce716f823",
   "metadata": {},
   "source": [
    "Images require special handling. We'll encode images to Base64 and generate summaries that describe their content, making them suitable for retrieval-based tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5630fc-779c-4caf-95da-395feda29220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode images to Base64\n",
    "def encode_image(image_path):\n",
    "    # Read and encode the image file\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d79a56a-1e87-4544-bcbc-f1e80b050d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate image summaries using the language model\n",
    "def image_summarize(img_base64, prompt):\n",
    "    chat = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb7a9a-68a5-4378-a1b7-6cd764733d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate summaries for all images in a directory\n",
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Lists to store Base64 encoded images and their summaries\n",
    "    img_base64_list = []\n",
    "    image_summaries = []\n",
    "\n",
    "    # Define the prompt for image summarization\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval.\n",
    "                Remember these images could potentially contain graphs, charts or tables also.\n",
    "                These summaries will be embedded and used to retrieve the raw image for question answering.\n",
    "                Give a detailed summary of the image that is well optimized for retrieval.\n",
    "                Do not add additional words like Summary: etc.\n",
    "             \"\"\"\n",
    "\n",
    "    # Process each image file in the directory\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "    return img_base64_list, image_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d56672-c1a0-41aa-b3d7-686ff0866232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing extracted images\n",
    "IMG_PATH = './figures'\n",
    "\n",
    "# Generate Base64 encoded images and their summaries\n",
    "imgs_base64, image_summaries = generate_img_summaries(IMG_PATH)\n",
    "\n",
    "# Display the number of images processed\n",
    "len(imgs_base64), len(image_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db23810d-64d3-41ac-bd9c-583b69a79e85",
   "metadata": {},
   "source": [
    "## 5. Building Vector Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a08838-f8ef-4042-a41c-f672fab506bf",
   "metadata": {},
   "source": [
    "Vector retrievers play a pivotal role in RAG by enabling efficient and relevant information retrieval. We'll build both multimodal and single-modal retrievers to handle diverse data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6184ce4b-cc8a-49e4-8a9d-28c5a21c603b",
   "metadata": {},
   "source": [
    "### 5.1. Access Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31dc4d-af44-429e-874f-716cf3fae399",
   "metadata": {},
   "source": [
    "We'll use OpenAI's embedding model to convert our summaries into vector representations suitable for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92c1769-9c0b-4906-bb46-f4cb070025ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI embedding model\n",
    "openai_embed_model = OpenAIEmbeddings(model='text-embedding-3-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06dbde8-7ca0-4cab-9aa5-a3f53a97af8f",
   "metadata": {},
   "source": [
    "### 5.2. Create Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833bf741-47fb-4e4e-b75c-e7011244c323",
   "metadata": {},
   "source": [
    "Utility functions will assist in managing documents and integrating them with our vector store and document store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b5e7d-c682-461e-be46-dc01f90fad00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a multi-vector retriever\n",
    "def create_multi_vector_retriever(\n",
    "    docstore, vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n",
    "):\n",
    "\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Initialize the MultiVectorRetriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=docstore,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the retriever\n",
    "    def add_documents(retriever, doc_summaries, doc_contents):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
    "\n",
    "    # Add text summaries and their contents\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    # Add table summaries and their contents\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    # Add image summaries and their contents\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c54ee41-f89f-47e1-a3b7-7073cc39b80f",
   "metadata": {},
   "source": [
    "### 5.3. Initiate Vectorstores: Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34839920-6a59-40f8-951e-844a7570d6d9",
   "metadata": {},
   "source": [
    "Chroma serves as our vector store, indexing the summaries and their embeddings for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9512b3fe-19de-4abf-82d8-f2f5d28c0a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Chroma vectorstore for multimodal data\n",
    "chroma_db_multimodal = Chroma(\n",
    "    collection_name=\"mm_rag\",\n",
    "    embedding_function=openai_embed_model,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5b3070-98a1-4081-ba0d-a4d881e6505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Chroma vectorstore for single-modal data\n",
    "chroma_db_single_modal = Chroma(\n",
    "    collection_name=\"text_rag\",\n",
    "    embedding_function=openai_embed_model,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b963834b-de05-4261-98d4-266c2bcf61a3",
   "metadata": {},
   "source": [
    "### 5.4. Initiate Docstores: Redis and InMemoryStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b4f62-b195-4fce-8631-b18fcc332538",
   "metadata": {},
   "source": [
    "Docstores store the raw documents corresponding to the summaries. We'll use Redis for the multimodal retriever and an in-memory store for the single-modal retriever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3503174b-2b76-48fd-9b6d-b07d5e3f575b",
   "metadata": {},
   "source": [
    "**Note:** Before proceeding, ensure that Redis Stack Server is installed and running. You can set it up by executing the following commands in JupyterLab's terminal:\n",
    "\n",
    "```bash\n",
    "# 1. Import the GPG key for the Redis repository\n",
    "curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg\n",
    "\n",
    "# 2. Add the Redis repository to your sources list\n",
    "echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] \\\n",
    "https://packages.redis.io/deb $(lsb_release -cs) main\" | \\\n",
    "sudo tee /etc/apt/sources.list.d/redis.list\n",
    "\n",
    "# 3. Update package lists\n",
    "sudo apt-get update\n",
    "\n",
    "# 4. Install Redis Stack Server\n",
    "sudo apt-get install redis-stack-server\n",
    "\n",
    "# 5. Start Redis Stack Server in the background\n",
    "redis-stack-server --daemonize yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f285b65-6a00-4a35-b1c9-0f0e8db94a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryStore\n",
    "from langchain_community.utilities.redis import get_client\n",
    "from langchain_community.storage import RedisStore\n",
    "\n",
    "# Initialize Redis client\n",
    "client = get_client('redis://localhost:6379')\n",
    "\n",
    "# Initialize RedisStore for multimodal retriever\n",
    "redis_store = RedisStore(client=client)  # Alternative stores like filestore or memorystore can also be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34435fa-b897-4a0d-826d-781a21cf580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize InMemoryStore for single-modal retriever\n",
    "docstore_single_modal = InMemoryStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b44fda0-425b-43a0-9fb1-a3a7ccd14c45",
   "metadata": {},
   "source": [
    "### 5.5. Create Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06b0aea-e717-48a8-af07-882287befe5a",
   "metadata": {},
   "source": [
    "With our vector stores and document stores set up, we'll create both multimodal and single-modal retrievers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ce1fb5-ec96-4fe6-8732-de026400e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the multimodal retriever\n",
    "retriever_multimodal = create_multi_vector_retriever(\n",
    "    redis_store,\n",
    "    chroma_db_multimodal,\n",
    "    text_summaries,\n",
    "    text_docs,\n",
    "    table_summaries,\n",
    "    table_docs,\n",
    "    image_summaries,\n",
    "    imgs_base64,\n",
    ")\n",
    "\n",
    "# Display the multimodal retriever\n",
    "retriever_multimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5993991b-3ad5-4db8-9ff7-d00cc3c333de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the single-modal retriever (only text)\n",
    "retriever_single_modal = create_multi_vector_retriever(\n",
    "    docstore_single_modal,\n",
    "    chroma_db_single_modal,\n",
    "    text_summaries,\n",
    "    text_docs,\n",
    "    table_summaries=[],  # No table summaries\n",
    "    tables=[],\n",
    "    image_summaries=[],  # No image summaries\n",
    "    images=[],\n",
    ")\n",
    "\n",
    "# Display the single-modal retriever\n",
    "retriever_single_modal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f326338-c592-4cc8-9d18-35264b44b284",
   "metadata": {},
   "source": [
    "## 6. Querying Exam Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4cd81e-b99c-4c8c-a5b3-c2bb70d11737",
   "metadata": {},
   "source": [
    "Now that our data is preprocessed and our retrievers are set up, we'll proceed to query exam questions using different RAG strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920e8e09-0fee-461e-878c-8464f6130b0d",
   "metadata": {},
   "source": [
    "### 6.1. Read the Excel File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9f0fc2-a290-4387-9337-18912dec4853",
   "metadata": {},
   "source": [
    "We'll start by loading the exam questions from an Excel file into a pandas DataFrame. This setup allows us to iterate through each question systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6666385d-18dc-4636-a50d-065f17de0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging to suppress unnecessary logs\n",
    "logging.basicConfig(level=logging.CRITICAL, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# Disable specific external library loggers to reduce clutter\n",
    "logging.getLogger('openai').disabled = True\n",
    "logging.getLogger('urllib3').disabled = True\n",
    "logging.getLogger('requests').disabled = True\n",
    "logging.getLogger('httpx').disabled = True\n",
    "\n",
    "# Create a dedicated logger for the application\n",
    "logger = logging.getLogger('EA_Exam_Processor')\n",
    "logger.setLevel(logging.WARNING)  # Only WARNING and above will be handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd962b4d-9a88-4a53-81af-2f56477b17df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Excel file containing exam questions\n",
    "excel_file_path = './EA QAs/EA QAs.xlsx'\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "questions_df = pd.read_excel(excel_file_path, sheet_name='EA QAs')\n",
    "\n",
    "# Ensure the columns are correctly loaded\n",
    "print(\"Columns in the Excel file:\", questions_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64faa2d-671c-44eb-861c-372c8e4a0401",
   "metadata": {},
   "source": [
    "### 6.2. Define Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608dffc1-b129-4eb6-9c2c-015dfa9844e5",
   "metadata": {},
   "source": [
    "Helper functions are essential for tasks like extracting answers, encoding images, resizing images, and determining the type of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b5cf45-a828-405f-8b48-7f40adfc28f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the answer from the model's response\n",
    "def extract_answer(text):\n",
    "    \"\"\"\n",
    "    Extracts the agent's answer, ensuring it is one of (A), (B), (C), or (D).\n",
    "    Returns the uppercase letter if found, otherwise logs a warning and returns None.\n",
    "    \"\"\"\n",
    "    # Normalize the text\n",
    "    text = text.strip().upper()\n",
    "    \n",
    "    # Use regex to find standalone A, B, C, or D, possibly followed by punctuation\n",
    "    match = re.search(r'\\b([ABCD])\\b', text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    # Additional checks: look for patterns like \"Answer: A\" or \"A)\"\n",
    "    match = re.search(r'ANSWER[:\\s]*([ABCD])', text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    match = re.search(r'\\b([ABCD])[).\\s]', text)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fbadb2-a055-44ec-a6d7-0ce8020edc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode the image to Base64\n",
    "def encode_image(image_path):\n",
    "    \"\"\"\n",
    "    Encodes an image file to a Base64 string after resizing it to a maximum size.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with PILImage.open(image_path) as img:\n",
    "            # Resize the image while maintaining aspect ratio\n",
    "            img.thumbnail((400, 400), PILImage.LANCZOS)\n",
    "            # Save the image to a BytesIO object\n",
    "            buffered = BytesIO()\n",
    "            img.save(buffered, format=\"JPEG\", quality=85)\n",
    "            # Encode the image to Base64 and decode to string\n",
    "            img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "            return img_str\n",
    "    except FileNotFoundError:\n",
    "        # Log a warning if the image file is not found\n",
    "        logger.warning(f\"Image file {image_path} not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        # Log any other exceptions\n",
    "        logger.warning(f\"Error encoding image {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd6af31-9d1b-487d-8668-08e86344953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to resize a Base64-encoded image\n",
    "def resize_base64_image(img_base64, max_size=(400, 400)):\n",
    "    \"\"\"\n",
    "    Resizes a Base64-encoded image to the specified maximum size.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img_data = base64.b64decode(img_base64)\n",
    "        img = PILImage.open(BytesIO(img_data))\n",
    "        img.thumbnail(max_size, PILImage.LANCZOS)\n",
    "        buffered = BytesIO()\n",
    "        img.save(buffered, format=\"JPEG\", quality=85)\n",
    "        resized_img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "        return resized_img_str\n",
    "    except Exception as e:\n",
    "        # Log any exceptions during resizing\n",
    "        logger.warning(f\"Error resizing image: {e}\")\n",
    "        return img_base64  # Return original if resizing fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243ac4fe-70e7-4d78-a7b1-c1c45f29809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a string looks like Base64\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d25b152-2d24-4455-bc6e-2bf116f4510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if Base64 data is an image\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xff\\xd8\\xff\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd7a8fe-679e-42cd-ac8a-a8fc4d9fe0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect if text contains a Markdown table\n",
    "def detect_markdown_table(text):\n",
    "    \"\"\"\n",
    "    Detects if the text contains a Markdown-formatted table.\n",
    "    \"\"\"\n",
    "    lines = text.strip().split('\\n')\n",
    "    if len(lines) >= 2:\n",
    "        # Check for header separator line (e.g., | --- | --- |)\n",
    "        header_line = lines[1].strip()\n",
    "        if re.match(r'^\\s*\\|?\\s*:-{1,}\\s*(\\|\\s*:-{1,}\\s*)+\\|?\\s*$', header_line):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0dc269-53fd-4e0f-bf5b-8e788913c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split documents into images, texts, and tables\n",
    "def split_docs_into_images_texts_tables(docs):\n",
    "    \"\"\"\n",
    "    Splits documents into images, texts, and tables.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    texts = []\n",
    "    tables = []\n",
    "    for doc in docs:\n",
    "        # Extract content and metadata\n",
    "        if isinstance(doc, Document):\n",
    "            content = doc.page_content\n",
    "            metadata = doc.metadata\n",
    "        else:\n",
    "            content = doc\n",
    "            metadata = {}\n",
    "    \n",
    "        # Ensure content is a string\n",
    "        if isinstance(content, bytes):\n",
    "            content = content.decode('utf-8', errors='ignore')\n",
    "    \n",
    "        # Extract category from metadata\n",
    "        category = metadata.get('category', '').lower()\n",
    "    \n",
    "        # Check if the document is a table based on metadata or content\n",
    "        if category == 'table':\n",
    "            tables.append({'content': content, 'metadata': metadata})\n",
    "            continue\n",
    "        elif '<table' in content.lower():\n",
    "            tables.append({'content': content, 'metadata': metadata})\n",
    "            continue\n",
    "        elif detect_markdown_table(content):\n",
    "            tables.append({'content': content, 'metadata': metadata})\n",
    "            continue\n",
    "    \n",
    "        # Remove data URL prefix if present\n",
    "        if content.startswith('data:image'):\n",
    "            content = content.split(',', 1)[1]\n",
    "    \n",
    "        # Check if content is an image\n",
    "        if looks_like_base64(content) and is_image_data(content):\n",
    "            images.append(content)\n",
    "        else:\n",
    "            texts.append(content)\n",
    "    return {'images': images, 'texts': texts, 'tables': tables}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa60802-88b0-4ad7-bb84-7ac68af9f21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to limit text length\n",
    "def limit_text_length(text, max_words=100):\n",
    "    \"\"\"\n",
    "    Truncates the input text to a maximum number of words.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text to truncate.\n",
    "    - max_words (int): The maximum number of words to retain.\n",
    "\n",
    "    Returns:\n",
    "    - str: The truncated text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    return ' '.join(words[:max_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c54d38-a94b-495b-8f99-1dc1bee9bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display a Base64-encoded image using matplotlib\n",
    "def display_base64_image(img_base64):\n",
    "    \"\"\"\n",
    "    Displays a Base64-encoded image using matplotlib.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img_data = base64.b64decode(img_base64)\n",
    "        img = PILImage.open(BytesIO(img_data))\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error displaying image: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342e5dd3-5328-4ba1-ab8e-4b66ab67f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make direct API calls with images\n",
    "def call_openai_api_with_image(messages, max_tokens=300):\n",
    "    \"\"\"\n",
    "    Makes a direct API call to OpenAI's Chat Completion endpoint with structured messages.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    if not api_key:\n",
    "        logger.error(\"OpenAI API key not set in environment variables.\")\n",
    "        return None\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.warning(f\"HTTP error occurred: {http_err} - Response: {response.text}\")\n",
    "    except Exception as err:\n",
    "        logger.warning(f\"Other error occurred: {err}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c363ec-678c-4d3e-8bfa-0689135ab524",
   "metadata": {},
   "source": [
    "### 6.3. Define RAG Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61c76db-77fa-4cf3-8eea-446fbe37414b",
   "metadata": {},
   "source": [
    "We'll define functions to handle both RAG and non-RAG strategies for answering multiple-choice questions. These functions manage the retrieval of relevant documents and interact with the language model to generate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e6f2bb-75d2-40e7-8e0c-c015ea41fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified RAG function using direct API calls with the updated invoke method\n",
    "def rag_multiple_choice_qa(question, options, retriever, image_base64=None):\n",
    "    \"\"\"\n",
    "    Answers a multiple-choice question using the given retriever.\n",
    "    Returns the answer (A, B, C, or D) and the sources.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents using the updated invoke method\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    \n",
    "    # Split documents into images, texts, and tables\n",
    "    sources = split_docs_into_images_texts_tables(retrieved_docs)\n",
    "    \n",
    "    # Limit text sources to 100 words\n",
    "    sources['texts'] = [limit_text_length(text) for text in sources['texts']]\n",
    "    \n",
    "    # Build the prompt\n",
    "    formatted_texts = \"\\n\".join(sources['texts'])\n",
    "    context_text = f\"Context documents:\\n{formatted_texts}\"\n",
    "    \n",
    "    # Build the prompt with clear instructions\n",
    "    prompt_text = f\"\"\"You are an assistant that answers multiple-choice questions based solely on the input provided.\n",
    "You will be given a question, several options, and context documents, which may include text, HTML tables, and images.\n",
    "Use the context documents to choose the correct option.\n",
    "Respond with only the letter of the chosen option: A, B, C, or D. Do not provide any additional text.\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\n",
    "Options:\n",
    "{options}\n",
    "\n",
    "{context_text}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Prepare the messages\n",
    "    messages = []\n",
    "    \n",
    "    # Include the question image if provided\n",
    "    if image_base64:\n",
    "        resized_image_base64 = resize_base64_image(image_base64)\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"User question includes an image.\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{resized_image_base64}\"}\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "    \n",
    "    # Add images from retrieved sources\n",
    "    for image_data in sources['images']:\n",
    "        resized_img_base64 = resize_base64_image(image_data)\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{resized_img_base64}\"}\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "    \n",
    "    # Add tables from retrieved sources\n",
    "    for table_dict in sources['tables']:\n",
    "        table_content = table_dict['content']\n",
    "        if isinstance(table_content, bytes):\n",
    "            table_content = table_content.decode('utf-8', errors='ignore')\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": table_content\n",
    "        })\n",
    "    \n",
    "    # Add the main prompt\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt_text\n",
    "    })\n",
    "    \n",
    "    # Make the API call\n",
    "    response_json = call_openai_api_with_image(messages, max_tokens=300)\n",
    "    \n",
    "    if not response_json:\n",
    "        return 'Invalid', sources\n",
    "    \n",
    "    # Extract the answer\n",
    "    try:\n",
    "        answer_text = response_json['choices'][0]['message']['content'].strip()\n",
    "        answer = extract_answer(answer_text)\n",
    "    except (KeyError, IndexError) as e:\n",
    "        logger.warning(f\"Error parsing response: {e} - Response: {response_json}\")\n",
    "        return 'Invalid', sources\n",
    "    \n",
    "    # If answer is None, log the issue and set to 'Invalid'\n",
    "    if answer is None:\n",
    "        logger.warning(f\"Invalid answer extracted for question: '{answer_text}'\")\n",
    "        # Optionally, implement a retry mechanism here\n",
    "        answer = 'Invalid'\n",
    "    \n",
    "    return answer, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f86d31c-cb60-4a25-8ba0-e9bbecdd7531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified No-RAG function using direct API calls\n",
    "def no_rag_multiple_choice_qa(question, options, image_base64=None):\n",
    "    \"\"\"\n",
    "    Answers a multiple-choice question without retrieval (no RAG).\n",
    "    Returns the answer (A, B, C, or D).\n",
    "    \"\"\"\n",
    "    # Build the prompt with clear instructions\n",
    "    prompt_text = f\"\"\"You are an assistant that answers multiple-choice questions based solely on the input provided.\n",
    "You will be given a question and several options.\n",
    "Choose the correct option from the given options, and respond with only the letter of the chosen option: A, B, C, or D. Do not provide any additional text.\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\n",
    "Options:\n",
    "{options}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Prepare the messages\n",
    "    messages = []\n",
    "    \n",
    "    # Include the question image if provided\n",
    "    if image_base64:\n",
    "        resized_image_base64 = resize_base64_image(image_base64)\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"User question includes an image.\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{resized_image_base64}\",\n",
    "                        \"detail\": \"high\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "    \n",
    "    # Add the main prompt\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt_text\n",
    "    })\n",
    "    \n",
    "    # Make the API call\n",
    "    response_json = call_openai_api_with_image(messages, max_tokens=300)\n",
    "    \n",
    "    if not response_json:\n",
    "        return 'Invalid'\n",
    "    \n",
    "    # Extract the answer\n",
    "    try:\n",
    "        answer_text = response_json['choices'][0]['message']['content'].strip()\n",
    "        answer = extract_answer(answer_text)\n",
    "    except (KeyError, IndexError) as e:\n",
    "        logger.warning(f\"Error parsing response: {e} - Response: {response_json}\")\n",
    "        return 'Invalid'\n",
    "    \n",
    "    # If answer is None, log the issue and set to 'Invalid'\n",
    "    if answer is None:\n",
    "        logger.warning(f\"Invalid answer: '{answer_text}'\")\n",
    "        # Optionally, implement a retry mechanism here\n",
    "        answer = 'Invalid'\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c55dbda-2247-4ba8-9a18-3272f51c0958",
   "metadata": {},
   "source": [
    "### 6.4. Process Each Question and Collect Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e4e67a-ded8-4988-91a4-992ef72c36f9",
   "metadata": {},
   "source": [
    "Finally, we'll iterate through each exam question, process it using different RAG strategies, and compare the generated answers against the true answers. We'll also display the sources retrieved by each retriever for transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d70acb2-1006-42af-a98e-9bff71eb85b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop to process the questions\n",
    "for idx, row in questions_df.iterrows():\n",
    "    question_number = row['#']\n",
    "    question_text = row['Question']\n",
    "    image_needed = row['Image']\n",
    "    options_text = row['Option']\n",
    "    true_answer = row['Answer']\n",
    "\n",
    "    # Load image if needed\n",
    "    image_base64 = None\n",
    "    if str(image_needed).strip().upper() == 'TRUE':\n",
    "        image_path = os.path.join('EA QAs', f\"{question_number}.jpg\")\n",
    "        image_base64 = encode_image(image_path)  # Adjust size as needed\n",
    "\n",
    "    # Multimodal RAG agent\n",
    "    mm_answer, mm_sources = rag_multiple_choice_qa(\n",
    "        question_text, options_text, retriever_multimodal, image_base64\n",
    "    )\n",
    "\n",
    "    # Single-modal RAG agent\n",
    "    sm_answer, sm_sources = rag_multiple_choice_qa(\n",
    "        question_text, options_text, retriever_single_modal, image_base64=image_base64\n",
    "    )\n",
    "\n",
    "    # No-RAG agent\n",
    "    nr_answer = no_rag_multiple_choice_qa(\n",
    "        question_text, options_text, image_base64\n",
    "    )\n",
    "\n",
    "    # Compare answers to true answer\n",
    "    mm_correct = 'CORRECT' if mm_answer == true_answer else 'WRONG'\n",
    "    sm_correct = 'CORRECT' if sm_answer == true_answer else 'WRONG'\n",
    "    nr_correct = 'CORRECT' if nr_answer == true_answer else 'WRONG'\n",
    "\n",
    "    # Display the results\n",
    "    print(f\"[{question_number}]\")\n",
    "    print(\"-\"*30)\n",
    "    print(f\"({true_answer}): True answer\\n\")\n",
    "    print(f\"({mm_answer}): Multimodal RAG - {mm_correct}\")\n",
    "    print(f\"({sm_answer}): Single-modal RAG - {sm_correct}\")\n",
    "    print(f\"({nr_answer}): No-RAG - {nr_correct}\")\n",
    "    print(\"-\"*30)\n",
    "    print(\"Question:\")\n",
    "    print(question_text)\n",
    "    if image_base64:\n",
    "        display_base64_image(image_base64)\n",
    "    print(\"\\nOptions:\")\n",
    "    print(options_text)\n",
    "    print(\"-\"*30)\n",
    "    \n",
    "    # Display Multimodal RAG Sources\n",
    "    print(\"Multimodal RAG Sources:\")\n",
    "    mm_text_sources = mm_sources['texts']\n",
    "    mm_image_sources = mm_sources['images']\n",
    "    mm_table_sources = mm_sources['tables']\n",
    "    \n",
    "    # Display Multimodal Text Sources\n",
    "    for i, text in enumerate(mm_text_sources):\n",
    "        print(f\"Text Source {i+1}:\")\n",
    "        display(Markdown(text))\n",
    "        print()\n",
    "    \n",
    "    # Display Multimodal Image Sources\n",
    "    for i, img_base64 in enumerate(mm_image_sources):\n",
    "        print(f\"Image Source {i+1}:\")\n",
    "        display_base64_image(img_base64)\n",
    "        print()\n",
    "    \n",
    "    # Display Multimodal Table Sources\n",
    "    for i, table_dict in enumerate(mm_table_sources):\n",
    "        print(f\"Table Source {i+1}:\")\n",
    "        table_content = table_dict['content']\n",
    "        # Try to parse and display as HTML table\n",
    "        try:\n",
    "            tables = pd.read_html(StringIO(table_content))\n",
    "            for table in tables:\n",
    "                display(table)\n",
    "        except ValueError:\n",
    "            # If parsing fails, try to render as Markdown table\n",
    "            try:\n",
    "                display(Markdown(table_content))\n",
    "            except Exception:\n",
    "                # If all else fails, display the raw content\n",
    "                print(table_content)\n",
    "        print()\n",
    "    \n",
    "    print(\"-\"*30)\n",
    "    \n",
    "    # Display Single-modal RAG Sources\n",
    "    print(\"Single-modal RAG Sources:\")\n",
    "    sm_text_sources = sm_sources['texts']\n",
    "    sm_image_sources = sm_sources['images']\n",
    "    sm_table_sources = sm_sources['tables']\n",
    "    \n",
    "    # Display Single-modal Text Sources\n",
    "    for i, text in enumerate(sm_text_sources):\n",
    "        print(f\"Text Source {i+1}:\")\n",
    "        display(Markdown(text))\n",
    "        print()\n",
    "    \n",
    "    # Display Single-modal Image Sources\n",
    "    for i, img_base64 in enumerate(sm_image_sources):\n",
    "        print(f\"Image Source {i+1}:\")\n",
    "        display_base64_image(img_base64)\n",
    "        print()\n",
    "    \n",
    "    # Display Single-modal Table Sources\n",
    "    for i, table_dict in enumerate(sm_table_sources):\n",
    "        print(f\"Table Source {i+1}:\")\n",
    "        table_content = table_dict['content']\n",
    "        # Try to parse and display as HTML table\n",
    "        try:\n",
    "            tables = pd.read_html(StringIO(table_content))\n",
    "            for table in tables:\n",
    "                display(table)\n",
    "        except ValueError:\n",
    "            # If parsing fails, try to render as Markdown table\n",
    "            try:\n",
    "                display(Markdown(table_content))\n",
    "            except Exception:\n",
    "                # If all else fails, display the raw content\n",
    "                print(table_content)\n",
    "        print()\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Add a 2-second pause to avoid rate limiting\n",
    "    time.sleep(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
